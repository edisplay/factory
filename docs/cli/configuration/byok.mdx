---
title: Bring Your Own Key
description: Connect your own API keys, use open source models, or run local models
keywords: ['byok', 'bring your own key', 'own api key', 'api key', 'custom models', 'local models', 'open source', 'self-hosted', 'use own key', 'personal key', 'own key']
---

Factory CLI supports custom model configurations through BYOK (Bring Your Own Key). Use your own OpenAI or Anthropic keys, connect to any open source model providers (like OpenRouter), or run models locally on your hardware. Once configured, switch between models using the `/model` command.

<Note>
  Your API keys remain local and are not uploaded to Factory servers. Custom models are only available in the CLI and won't appear in Factory's web or mobile platforms.
</Note>

<img src="/images/custom_models.png" alt="Model selector showing custom models" />

[Install the CLI with the 5-minute quickstart →](/cli/getting-started/quickstart)

---

## Configuration Reference

Add custom models to `~/.factory/settings.json` under the `customModels` array:

```json
{
  "customModels": [
    {
      "model": "your-model-id",
      "displayName": "My Custom Model",
      "baseUrl": "https://api.provider.com/v1",
      "apiKey": "${PROVIDER_API_KEY}",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 16384
    }
  ]
}
```

<Tip>
  In `settings.json` (and `settings.local.json`), `apiKey` supports environment variable references using `${VAR_NAME}` syntax. For example, `"apiKey": "${PROVIDER_API_KEY}"` reads from the environment variable named `PROVIDER_API_KEY` (for example: `export PROVIDER_API_KEY=your_key_here`).
</Tip>

<Note>
  **Legacy support**: Custom models in `~/.factory/config.json` using snake_case field names (`custom_models`, `base_url`, etc.) are still supported for backwards compatibility. Both files are loaded and merged, with `settings.json` taking priority. Env var expansion for `apiKey` applies to `settings.json`/`settings.local.json` and not to legacy `config.json`.
</Note>

### Supported Fields

| Field | Required | Description |
|-------|----------|-------------|
| `model` | ✓ | Model identifier sent via API (e.g., `claude-sonnet-4-5-20250929`, `gpt-5-codex`, `qwen3:4b`) |
| `displayName` | | Human-friendly name shown in model selector |
| `baseUrl` | ✓ | API endpoint base URL |
| `apiKey` | ✓ | Your API key for the provider. Can't be empty. Supports `${VAR_NAME}` in `settings.json`/`settings.local.json` (e.g., `${PROVIDER_API_KEY}` uses the `PROVIDER_API_KEY` environment variable). |
| `provider` | ✓ | One of: `anthropic`, `openai`, or `generic-chat-completion-api` |
| `maxOutputTokens` | | Maximum output tokens for model responses |

---

## Understanding Providers

Factory supports three provider types that determine API compatibility:

| Provider | API Format | Use For | Documentation |
|----------|------------|---------|---------------|
| `anthropic` | Anthropic Messages API (v1/messages) | Anthropic models on their official API or compatible proxies | [Anthropic Messages API](https://docs.claude.com/en/api/messages) |
| `openai` | OpenAI Responses API | OpenAI models on their official API or compatible proxies. Required for the newest models like GPT-5 and GPT-5-Codex. | [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) |
| `generic-chat-completion-api` | OpenAI Chat Completions API | OpenRouter, Fireworks, Together AI, Ollama, vLLM, and most open-source providers | [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat) |

<Warning>
  Factory is actively verifying Droid's performance on popular models, but we cannot guarantee that all custom models will work out of the box. Only Anthropic and OpenAI models accessed via their official APIs are fully tested and benchmarked.
</Warning>

---

## Prompt Caching

Factory CLI automatically uses prompt caching when available to reduce API costs:

- **Official providers (`anthropic`, `openai`)**: Factory attempts to use prompt caching via the official APIs. Caching behavior follows each provider's implementation and requirements.
- **Generic providers (`generic-chat-completion-api`)**: Prompt caching support varies by provider and cannot be guaranteed. Some providers may support caching, while others may not.

### Verifying Prompt Caching

To check if prompt caching is working correctly with your custom model:

1. Run a conversation with your custom model
2. Use the `/cost` command in Droid CLI to view cost breakdowns
3. Look for cache hit rates and savings in the output

If you're not seeing expected caching savings, consult your provider's documentation about their prompt caching support and requirements.

---

## Quick Start Examples

### Bring Your Own OpenAI/Anthropic Keys

Use your own API keys for cost control and billing transparency:

```json
{
  "customModels": [
    {
      "model": "claude-sonnet-4-5-20250929",
      "displayName": "Sonnet 4.5 [Custom]",
      "baseUrl": "https://api.anthropic.com",
      "apiKey": "YOUR_ANTHROPIC_KEY",
      "provider": "anthropic"
    },
    {
      "model": "gpt-5-codex",
      "displayName": "GPT5-Codex [Custom]",
      "baseUrl": "https://api.openai.com/v1",
      "apiKey": "YOUR_OPENAI_KEY",
      "provider": "openai"
    }
  ]
}
```

### Access via OpenRouter

Connect to OpenRouter for access to models from multiple providers:

```json
{
  "customModels": [
    {
      "model": "openai/gpt-oss-20b",
      "displayName": "GPT-OSS-20B",
      "baseUrl": "https://openrouter.ai/api/v1",
      "apiKey": "YOUR_OPENROUTER_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 32000
    }
  ]
}
```

Browse available models at [openrouter.ai/models](https://openrouter.ai/models).

### Fireworks AI

Access high-performance inference for open-source models with optimized serving:

```json
{
  "customModels": [
    {
      "model": "accounts/fireworks/models/glm-4p5",
      "displayName": "GLM 4.5 [Fireworks]",
      "baseUrl": "https://api.fireworks.ai/inference/v1",
      "apiKey": "YOUR_FIREWORKS_API_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 16384
    },
    {
      "model": "accounts/fireworks/models/llama-v3p1-405b-instruct",
      "displayName": "Llama 3.1 405B [Fireworks]",
      "baseUrl": "https://api.fireworks.ai/inference/v1",
      "apiKey": "YOUR_FIREWORKS_API_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 131072
    }
  ]
}
```

Get your API key at [fireworks.ai](https://fireworks.ai). Browse available models in their [model catalog](https://fireworks.ai/models).

### Baseten

Deploy and serve custom models with enterprise-grade infrastructure:

```json
{
  "customModels": [
    {
      "model": "YOUR_MODEL_ID",
      "displayName": "Custom Model [Baseten]",
      "baseUrl": "https://inference.baseten.co/v1",
      "apiKey": "YOUR_BASETEN_API_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 8192
    },
    {
      "model": "llama-3.1-70b-instruct",
      "displayName": "Llama 3.1 70B [Baseten]",
      "baseUrl": "https://inference.baseten.co/v1",
      "apiKey": "YOUR_BASETEN_API_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 131072
    }
  ]
}
```

<Note>
  Baseten supports custom model deployments. Replace `YOUR_MODEL_ID` with your deployed model's ID from the Baseten dashboard.
</Note>

### DeepInfra

Access cost-effective inference for a wide variety of open-source models:

```json
{
  "customModels": [
    {
      "model": "zai-org/GLM-4.7",
      "displayName": "GLM 4.7 [DeepInfra]",
      "baseUrl": "https://api.deepinfra.com/v1/openai",
      "apiKey": "YOUR_DEEPINFRA_TOKEN",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 16384
    },
    {
      "model": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "displayName": "Qwen 2.5 Coder 32B [DeepInfra]",
      "baseUrl": "https://api.deepinfra.com/v1/openai",
      "apiKey": "YOUR_DEEPINFRA_TOKEN",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 32768
    },
    {
      "model": "deepseek-ai/DeepSeek-V3",
      "displayName": "DeepSeek V3 [DeepInfra]",
      "baseUrl": "https://api.deepinfra.com/v1/openai",
      "apiKey": "YOUR_DEEPINFRA_TOKEN",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 65536
    }
  ]
}
```

Get your API token at [deepinfra.com](https://deepinfra.com). View available models at their [model list](https://deepinfra.com/models).

### Hugging Face Inference API

Connect to models hosted on Hugging Face's Inference API:

```json
{
  "customModels": [
    {
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "displayName": "Mistral 7B [HF]",
      "baseUrl": "https://api-inference.huggingface.co/models",
      "apiKey": "YOUR_HF_TOKEN",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 32768
    },
    {
      "model": "codellama/CodeLlama-13b-Instruct-hf",
      "displayName": "CodeLlama 13B [HF]",
      "baseUrl": "https://api-inference.huggingface.co/models",
      "apiKey": "YOUR_HF_TOKEN",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 16384
    }
  ]
}
```

<Note>
  Get your Hugging Face token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens). Some models may require Pro subscription for faster inference.
</Note>

### Ollama (Local and Cloud)

#### Local Ollama

Run models locally on your hardware with Ollama:

```json
{
  "customModels": [
    {
      "model": "qwen2.5-coder:7b",
      "displayName": "Qwen 2.5 Coder [Local]",
      "baseUrl": "http://localhost:11434/v1",
      "apiKey": "not-needed",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 8192
    },
    {
      "model": "deepseek-coder-v2:16b",
      "displayName": "DeepSeek Coder V2 [Local]",
      "baseUrl": "http://localhost:11434/v1",
      "apiKey": "not-needed",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 16384
    }
  ]
}
```

<Tip>
  First, install Ollama and pull your desired model:
  ```bash
  ollama pull qwen2.5-coder:7b
  ollama serve  # Start the local server
  ```
</Tip>

#### Ollama Cloud

Use Ollama's cloud service for hosted model inference:

```json
{
  "customModels": [
    {
      "model": "llama3.1:8b",
      "displayName": "Llama 3.1 8B [Ollama Cloud]",
      "baseUrl": "https://ollama.com",
      "apiKey": "YOUR_OLLAMA_API_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 131072,
      "extraHeaders": {
        "Authorization": "Bearer YOUR_OLLAMA_API_KEY"
      }
    }
  ]
}
```

<Note>
  For Ollama Cloud, you'll need to include the Authorization header with your API key. Get your API key from [ollama.com](https://ollama.com).
</Note>

### Connect to Google Gemini

Access Google's Gemini models using your Gemini AI API key:

```json
{
  "customModels": [
    {
      "model": "gemini-2.5-pro",
      "displayName": "Gemini 2.5 Pro",
      "baseUrl": "https://generativelanguage.googleapis.com/v1beta/",
      "apiKey": "YOUR_GEMINI_API_KEY",
      "provider": "generic-chat-completion-api",
      "maxOutputTokens": 32000
    }
  ]
}
```



---

## Using Custom Models

Once configured, access your custom models in the CLI:

1. Use the `/model` command
2. Your custom models appear in a separate "Custom models" section below Factory-provided models
3. Select any model to start using it

Custom models display with the name you set in `displayName`, making it easy to identify different providers and configurations.

---

## Troubleshooting

### Model not appearing in selector
- Check JSON syntax in `~/.factory/settings.json` (or `config.json` if using legacy format)
- Settings changes are detected automatically via file watching
- Verify all required fields are present

### "Invalid provider" error
- Provider must be exactly `anthropic`, `openai`, or `generic-chat-completion-api`
- Check for typos and ensure proper capitalization

### Authentication errors
- Verify your API key is valid and has available credits
- Check that the API key has proper permissions
- Confirm the base URL matches your provider's documentation

### Local model won't connect
- Ensure your local server is running (e.g., `ollama serve`)
- Verify the base URL is correct and includes `/v1/` suffix if required
- Check that the model is pulled/available locally

### Rate limiting or quota errors
- Check your provider's rate limits and usage quotas
- Monitor your usage through your provider's dashboard

---

## Billing

- You pay your provider directly with no Factory markup or usage fees
- Track costs and usage in your provider's dashboard
